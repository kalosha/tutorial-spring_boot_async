რას ვაჩვენებთ დემონსტრაციაზე:

აქ არ არის საუბარი თუ როგორ არის ეს პროექტი იმპლემენტირებული, რათქმაუნდა, ბევრი სხვა მიდგომა იქნება სწორი, მაგალითად endpoint-ების ოპტიმიზაცია, bulk გამოძახებები, კეშირება და ა.შ.
აქ საუბარია თუ რას უნდა მივაქციოთ ყურადღება როცა ვწერთ CompletableFuture-ის გამოყენებით. ვინაიდან ხშირად შემხევდრია მიდგომა რომ გამოვიყენებ CompletableFuture და მრავალ ნაკადიანობა ხდება უფრო მარტივი. არა არ არის ეს სიმართლე, ამას ვაწყდებოდი და ვაწყდები ყოველდღიურად ბევრ სხვადასხვა პროექტში.

ვრთავთ zipkin-ს და ვხსნით შესაბამისად http://localhost:8411/zipkin/
1. ვუხნით პროექტი როგორ ავაწყვეთ
	1.1 DownstreamServer - რას აკეთებს რომ ააქვს რამოდენიმე ენდპოინტი: states, dealers, manufacturers
	1.2 MiddleSvc - რას აკეთებს, აზრი რა არის ამ სერვისის, რომ ის აგროვებს ინფორმაციას სხვა სერვერებიდან და უბრინებს მომხმარებელს. 
		ამ შენთხვევაში ყველაზე იაფი მანქანების სიას შტატში

2. თავიდან ვაჩვენებთ მარტივ მაგალითს:
	2.1 პირდაპირ იმპლემენტაციას ვანახებთ. ვუხსნით როგორ არის იმპლემენტირებული, ვანხებთ სქემას. 
	2.2 ვაკეთებთ დემონსტრაციას როგორ მუშაობს swagger-იდან
	2.3 ვსტარტავთ DownstreamServer სერვისს შესაბამისი პარამეტრებით:
					-Dcom.epam.sleepTime=10					// დაყოვნება მილიწამებში ყველა ენდპოინტზე მიმდინარე სერვისზე
					-Dserver.tomcat.threads.max=5			// მაქსიმალური თრედების რაოდენობა რაც შეუძლია გახსნას სერვისმა
					-Dserver.tomcat.threads.min-spare=1		// მინიმალური რაოდენობა თრედების როა დატვითვა არ არის, შესაბამისად თAვისით ანადგურებს სხვა თრედებს
	2.4 კიდევ ერთხელ დემონსტრირება swagger-იდან
	2.5 ვსტარტავთ VisualVM-ს და შესაბამისად ვანახებთ თრედების მოქმედებას.
	2.6 ვსტარტავთ JMeter-ს
		2.6.1 თავიდან ვანახებთ მომხმარებლების რაოდენობის მიედვით
			როცა მომხმარებელი 1 ცალია და იძახებს 1-ჯერ, ვანახებთ რეპორტების შედეგებს (ცხრილის სახით)
			როცა მომხმარებელი 1 ცალია და იძახებს 5-ჯერ, ვანახებთ რეპორტების შედეგებს (ცხრილის სახით) ყურადღებას ვამახვილებთ რომ მარტო ერთი თრედი გამოიყენება
			როცა მომხმარებელი 1 ცალია და იძახებს 20-ჯერ, ვანახებთ რეპორტების შედეგებს (ცხრილის სახით) ყურადღებას ვამახვილებთ რომ ჯერ კიდევ მარტო ერთი თრედი გამოიყენება
			
			როცა მომხმარებელი 2 ცალია და იძახებს 1-ჯერ, ვანახებთ რეპორტების შედეგებს (ცხრილის სახით) ყურადღებას ვამახვილებთ რომ ახალი თრედი გაიხსნა. მაგრამ პასუხის დრო იგივეა
			როცა მომხმარებელი 2 ცალია და იძახებს 20-ჯერ, ვანახებთ რეპორტების შედეგებს (ცხრილის სახით) ყურადღებას ვამახვილებთ რომ მატო 2 თრედია. მაგრამ პასუხის დრო იგივეა
			
			როცა მომხმარებელი 5 ცალია და იძახებს 1-ჯერ, ვანახებთ რეპორტების შედეგებს (ცხრილის სახით) ყურადღებას ვამახვილებთ რომ მარტო 5 თრედია. მაგრამ პასუხის დრო იგივეა
			როცა მომხმარებელი 5 ცალია და იძახებს 20-ჯერ, ვანახებთ რეპორტების შედეგებს (ცხრილის სახით) ყურადღებას ვამახვილებთ რომ მატო 5 თრედია. მაგრამ პასუხის დრო იგივეა
			
			როცა მომხმარებელი 6 ცალია და იძახებს 1-ჯერ, ვანახებთ რეპორტების შედეგებს (ცხრილის სახით) ყურადღებას ვამახვილებთ ჩვენ შეზღუდვა გვაქვს თრედების რაოდენობაზე 5. 
				აქ ვაჭარბებთ უკვე 1 თრედით. შესაბამისად პასუხის დრო ყველა 6 მომხმარებლის პასუხზე გაიზარდა.
			როცა მომხმარებელი 6 ცალია და იძახებს 20-ჯერ, ვანახებთ რეპორტების შედეგებს (ცხრილის სახით) პასუხის დრო კვლავ ყველა მოთხოვნაზე გაზრდილია. და იგივეა რაც ერთი მოთხოვნის გაგზავნისას.
			
			კვლავ ვზრდით მომხმარებელ +1 და ვაკვირდებით შედეგს. ყველასთვის პასუხის დრო კვლავ გაიზარდა.
			კვლავ ვზრდით მომხმარებელ +1 და ვაკვირდებით შედეგს. ყველასთვის პასუხის დრო კვლავ გაიზარდა.
			
			ეხლა მომხმარებლების რაოდენობას ვზრდით 20-მდე და ვაკვირდებით როგორ დრამატულად იმატებს პასუხის დრო.
			
	2.7 ვსტარტავთ უკვე 3 DownstreamServer სერვისს სხვადასხვა პორტებზე.
			შეგვიძლია იგივე ცდები ჩავატაროთ მომხმარებლებზე და გამოძახებების რაოდენობაზე და ვნახავთ რომ:
				დროები დაახლოებით იგივეა
				და მნიშვნელოვანია შეფარდება მაქსიმალური თრედების რაოდენობისა და მომხამრებლებისა ანუ რექვესტებისა.
			რაც იმას ნიშნავს რომ ამ ყველაფერის უფრო ზუსტი კონფიგურირება უნდა გაკეთდეს და სასურველია DevOp-ების ჩართულობით. და ეს მნიშვნელოვანია ვინაიდან უნდა ვიცეოდეთ დატვირთვები, POD-ბის პროცესორების სიმძლავრე (მეხსიერებას ამ ეტაპზე არ ვეხები მაგრამ ეგეც რათქმაუნდა მნიშვნელოვანია)
			
3. გადავდივართ პირველად მავალნაკადიან ვარიანტზე ერთი Executor-ით ( აქ მისათითებელია რომ @EnableAsync არ ვიყენებთ)
			
4. გადავდივართ მეორე მავალნაკადიან ვარიანტზე @Async-ის და @EnableAsync-ის გამოყენებით
    4.1 აქ კიდევ ერთი პრობლემაა. რა არის როგორ ფიქრობთ?
    4.2 დემონსტრირება ამ პსობლემის (ციკლში იგივე Executor-ის გამოყენება)

5. მულტი-თრედინგის ბნელი მხარეები
	5.1 უფრო რთული კოდი, დასაწერად და შემდგომი ანალიზისთვის.
		5.1.1 დებაგირების პრობლემები.
	5.2 რესურსების უფრო აგრესიული გამოყენება
	5.3 მიკროსერვისების შემთხვევში შეიძლება ამან გამოიწვიოს სხვა სერვისების ჩავარდნა, მიუხიდავად იმისა პირდაპირ ვიძახებთ ამ სერვისებს თუ არა.
		5.3.1 მაგალითად მიკროსერვისების გარემოში შესაძლებელია ერთ და იგივე A სერვერს აკითხავდეს ბევრი სხვა სერვისები [B1,B2 ... Bn], თუ სერვიცი C1 იყენებს B სერვისების რაღაც რაოდენობას, აქედან გამოდის რომ A სერვისზე დატვირთვა იზრდება ექსპონენციალურად.
				
				   /->B1-\
				  //->B2-\\
			  C1----->B3---->A    თუ C1 სერვისის გამოძახება ხდება ციკლში, შეგიძლიათ რა ხდება მაშინ.
				  \\->B4-//
				   \->B5-/